\section{Neural Nets}

  \begin{itemize}
    \item Composed of hidden layers between input and output
  \end{itemize}

  \subsection{Types of Functions}

    \begin{itemize}
      \item \textbf{One layer}: linear decision
      \item \textbf{A hidden layer}: any continuous function
      \item \textbf{More layers}: finer boundaries
      \item Flatter layers are harder to design
    \end{itemize}

  \subsection{Feedforward Networks}

  \subsection{Convolutional Networks}

  \subsection{Generative Adversarial Networks}

    \begin{itemize}
      \item Composed of two neural nets:
      \begin{itemize}
        \item One generates fake data
        \item The other one decides if the image (real and fake) it sees is fake
      \end{itemize}

      \item Can generate realistic local fatures, but gets the big picture
      wrong:
      \begin{itemize}
        \item Too many eyes
        \item Wrong hands
      \end{itemize}
    \end{itemize}

  \subsection{Recurrent Networks (Layers)}

    \begin{itemize}
      \item \ul{Each \textbf{time stamp}}:
      \begin{itemize}
        \item \ul{Produce two outputs}: an output, and a state
        \item \ul{Takes two inputs}: an input and a previous state
        \item \ul{Loss is calculated from prediction}
      \end{itemize}

      \item May have a number of time stamps in on execution of the layer
      \item Total loss is accmulated through time
      \item Can be think of as a state machine
      \item All neurons are fully inter-connected
      \item Back-propogate through time
      \begin{itemize}
        \item Hard to train
      \end{itemize}

      \item \textbf{Sentence processing}: A time stamp is executed
      for every word
    \end{itemize}

    \subsubsection{Gating}

      \paragraph{Problem} signals from one time stamp run decay over time;
      \ul{decay too fast for linguistic applications}

      \paragraph{Solution} uses a gating vector that controls what is
      forgotten and how fast; \ul{popular implementations}:
      \begin{itemize}
        \item LSTM
        \item Long short term memory
        \item GRU (Gold recurrent unit)
      \end{itemize}

    \subsubsection{Design}

      \begin{itemize}
        \item Typically very shallow
        \item Can have non-linear processing attached at the end of one
      \end{itemize}

    \subsubsection{Execution}

    \subsubsection{Bidirectional RNN}

      \begin{itemize}
        \item Two RNN working in opposite directions
        \item Generate a summary output from the output from the two networks
        \item Use the same loss to optimize both networks
      \end{itemize}

    \subsubsection{Linguistic Application}

      \begin{itemize}
        \item
        \item \textbf{Bidirectional RNN}
      \end{itemize}