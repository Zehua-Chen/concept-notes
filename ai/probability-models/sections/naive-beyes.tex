\section{Naive Beyes}

  \begin{align}
    P\left( B | A \right)
      &= P\left( A | B \right) \frac{P\left( B \right)}{P\left( A \right)} \\
    P\left( \text{cause} | \text{evidence} \right)
      &= P\left( \text{evidence} | \text{cause} \right)\frac{P\left( \text{cause} \right)}{P\left( \text{evidence} \right)}
  \end{align}

  \begin{itemize}
    \item \textbf{Posterior}: $ P\left( \text{cause} | \text{evidence} \right) $
    \item \textbf{Likelihood}: $ P\left( \text{evidence} | \text{cause} \right) $
    \item \textbf{Normalization}: $ P\left( \text{evidence} \right) $
    \item \textbf{Prior}: $ P\left( \text{cause} \right) $
  \end{itemize}

  Want to compute posterior from likelihood, normalization and prior,
  which are easier to obtain than posterior. Setting up the algorithm this
  way would also helps with adjusting for $ P\left( \text{cause} \right) $

  \subsection{Ignoring the Normalization}

    Since in most cases, we just want to know either the probability ratio or
    the biggest probability, in which case $ P\left( \text{evidence} \right) $
    is always the same, we can ignore $ P\left( \text{evidence} \right) $
    from the Beyes equation

    \begin{equation}
      P\left( \text{cause} | \text{evidence} \right)
        \propto P\left( \text{evidence} | \text{cause} \right) P\left( \text{cause} \right)
    \end{equation}

  \subsection{Maximum a Posteriori (MAP)}

    To find a cause that leads to an evidence, pick a type $ X $ from a set of
    types $ T $ such that:

    \begin{equation}
      X = \argmax_{x \in T} P\left( x | \text{evidence} \right)
    \end{equation}

  \subsection{Maximum Likelihood Estimate (MLE)}

    If we know all causes are equally likely, we can set $ P\left( \text{cause} \right) $
    to $ 1 $

    \begin{equation}
      P\left( \text{cause} | \text{evidence} \right)
        \propto P\left( \text{evidence} | \text{cause} \right)
    \end{equation}

    \begin{itemize}
      \item \ul{Can be very inaccurate if priors are different}
      \item \ul{Sensible} choice if we have \ul{poor information about
      prior probabilities}
    \end{itemize}

  \subsection{Naive Beyes Model}

    We have evidence $ A $, $ B $ related to cause $ C $. So we have
    \begin{displaymath}
      P\left( C | A, B \right) \propto P\left( A, B | C \right) \cdot P\left( C \right)
    \end{displaymath}

    Suppose $ A $ and $ B $ are conditionally independent given $ C $. Then
    \begin{displaymath}
      P\left( A, B | C \right) = P\left( A | C \right) * P\left( B | C \right)
    \end{displaymath}

    Subsituting, we get
    \begin{align}
      P\left( C | A, B \right) &\propto P\left( A | C \right) * P\left( B | C \right) \cdot P\left( C \right) \\
      P\left( C | E_{1} ... E_{n} \right) &\propto P\left( C \right) \cdot \sum_{k = 1}^{n} P\left( E_{k} | C \right)
    \end{align}

    With this, we can estimate the \ul{relationship to the cause separately
    for each type of evidence, then combine information to get the MAP
    estimate}.

    \subsubsection{Size of Naive Beyes Model vs Full Joint Distribution}

      \begin{itemize}
        \item A naive beyes model only has $ O\left( n \right) $ models to
        estiamte
        \item A full joint distribution model has $ 2^{n} $ parameters to
        estiamte
      \end{itemize}

  \subsection{Examples}

    \subsubsection{Bike}

      We saw a teal bike, what brand (standard, veodride) is it?

      $ P(veo | teal) = \frac{P(teal | veo) P(veo)}{P(teal)} = 0.905 $

      $ P(standard | teal) = \frac{P(teal | standard) P(standard)}{P(teal)} = 0.095 $

      \paragraph{Assign Classes}

      $ \underset{c\in C_{0}, ..., C_{n}}{\operatorname{argmax}} P( C_{n} | evidence) $

      Find the class in which the object most likely belongs to (**maximize posterior**).

      \paragraph{Maximum Likelihood Estimate (MLE Estimate)}

    \subsubsection{Bag of Words}

      \begin{itemize}
        \item Treat words independently
        \begin{itemize}
          \item No phrases (ex. good people, can't recognize "not x")
          \item No word order
        \end{itemize}

        \item Use the word with the highest probability as the class
      \end{itemize}

      \paragraph{Steps}
      \begin{enumerate}
        \item Convert input string to a clean string of words
        \begin{itemize}
          \item Divide at whitespace
          \item Normalize punctuations (capitalization, wierd characters, ...)
          \item Put words with the same base together (helping, helped, help)
        \end{itemize}

        \item Find $ P(w | class) $ and $ P (w | uk) $, perform naive estimate:
      \end{enumerate}

      \paragraph{Problems: Underflow}
      \begin{itemize}
        \item All numbers are small
        \begin{itemize}
          \item Each $ P ( w| class) $ is small
          \item Use multiple passes
        \end{itemize}

        \item Use logs (converts mutliplications to sums)
      \end{itemize}

      \paragraph{Problem: Overfitting}
      \begin{itemize}
        \item Common words should have similar frequencies
        \item Rare words may appear in one set but not the other
        \begin{itemize}
          \item Each one is rare
          \item Foreign words
          \item Proper names
        \end{itemize}
      \end{itemize}

      \paragraph{Smoothing}
      \begin{itemize}
        \item Give non-zero probability to rare words
        \item Probability mass: probability viewed as stuff
        \begin{itemize}
          \item Way too many methods
          \item All "empirical": not well-backed by theory
        \end{itemize}
      \end{itemize}

      \paragraph{Laplace Smoothing}
      \begin{align*}
        n &= \text{number of words in our data} \\
        count(w) &= \text{number of times w was seen} \\
        v &= \text{number of word types} \\
        alpha &= \text{magic constant (tunable parameter)} \\
        P(w) &= (count(w) + alpha) / (n + alpha (v + 1)) \\
        P(unk) &= (alpha) / (n + alpha (v + 1))
      \end{align*}

      \paragraph{Problems When Dealing with Non-English}
      \begin{itemize}
        \item No space between words: find word segements
        \begin{itemize}
          \item Chinese
        \end{itemize}

        \item Long words: divide up complex words
        \begin{itemize}
          \item German: Winter-coat
        \end{itemize}
      \end{itemize}

      \paragraph{Problems Dealing With English}
      \begin{itemize}
        \item Common words: remove, not giving useful information
        \begin{itemize}
          \item Stop word: a, the, in, arm
        \end{itemize}

        \item Very rare words: delete or translate into common place holder, i.e. UNK)
        \begin{itemize}
          \item Hard to measure probabilities
          \item Lots of them; takes up memory
        \end{itemize}
      \end{itemize}

      \paragraph{Phrases}
      Group words into a set
      \begin{itemize}
        \item Bigram: \say{big cat}
        \item Trigram...
      \end{itemize}

      Words set make training harder to do
      \begin{itemize}
        \item Sparser training data
        \item Make need for smoothing
      \end{itemize}

      \subparagraph{Solution 1}
      Calculate probabilities from word probabilities and context probabilities.
      Ex. given \say{The angry X}. Some words can be X more in come context

      \subparagraph{Solution 2}
      \textbf{Fertile contexts}: if we have seen a lot of words in some context,
      we are likely to see more words there
      Ex. given \say{give me the X}, there can be a lot of \say{X}

  \subsection{Early Use}

    \subsubsection{Spam Cops (1998)}

      \begin{itemize}
        \item 4.33 error rate
        \item O(n) parameters
        \item O(n + k)
      \end{itemize}

    \subsubsection{Fishser Corpus}

      \begin{itemize}
        \item 16000 transcribed phone conversations
        \item Try to tell gender of speaker from words used (bag of word problem)
        \begin{itemize}
          \item Filler words (ah)
          \item Function words (at, and)
        \end{itemize}
      \end{itemize}