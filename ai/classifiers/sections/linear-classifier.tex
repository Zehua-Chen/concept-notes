\section{Linear Classifier}

  \subsection{What is Linearly Separatable}

    A dataset is linearly separable if we can \ul{draw a line that separates
    one class from another}

    \begin{itemize}
      \item In most cases, a dataset is \textbf{almost linearly separable},
      which means that most instances of the classes can be separated from
      one another using a line, but not all of them
    \end{itemize}

  \subsection{Perceptrons}

    Each neuron takes an input vector $ x $ and have a weight number for
    each of the component in the input vector $ w $, and procduces a
    decision using the \textbf{activation function}.

    \subsubsection{Activation Function}

      An activation function produces the decision of a neuron.

    \subsubsection{Loss Function}

      \begin{equation}
        \left( y - p \right)^{2}
      \end{equation}

      \paragraph{Cross Entroy Loss}
      \begin{equation}
        -\left( y \log P + \left( 1 - y \right) \log\left( 1 - P \right) \right)
      \end{equation}

  \subsection{Neural Nets}

    \begin{itemize}
      \item Composed of hidden layers between input and output
    \end{itemize}

    \subsubsection{Types of Functions}

      \begin{itemize}
        \item One layer: linear decision
        \item A hidden layer: any continuous function
        \item More layers: finer boundaries
      \end{itemize}

    \subsubsection{Layers}

      \begin{itemize}
        \item Flatter layers are harder to design
      \end{itemize}